Massachusetts  Institute  of  Technology 
6.867  Machine  Learning,  Fall  2006 

Problem  Set  2:  Solutions 

1.	

(a)  (5  points)  From  the  lecture  notes  (Eqn  14,  Lecture  5),  the  optimal  parameter  values  for  linear 
regression  given  the matrix  of  training  examples X  and  the  corresponding  response  variables  y  is: 
θ = (XT X)−1XT y 
The quantity (XT X)−1XT  is also known as the pseudo-inverse of X, and often occurs when dealing 
with  linear  systems  of  equations.  When X  is  a  square matrix  and  invertible,  it  is  exactly  the  same 
as  the  inverse  of X. 
MATLAB provides many ways of achieving our desired goal.  We  can directly write out  the  expres­
sion  above  or  use  the  function  pinv.  Here  are  the  functions  linear  regress  and  linear  pred: 

function  theta  =  linear_regress(y,X) 

theta  =  pinv(X)*y;


function  y_hat  =  linear_pred(theta,X)

y_hat  =  X*theta;


In linear  regress, note that we are not calculating θ0  separately.  This diﬀers from the description 
in  the  lecture  notes  where  the  training  examples  were  explicitly  padded  with  1’s,  allowing  us  to 
introduce  an  oﬀset  θ0 .  Instead,  we  will  use  a  feature  mapping  to  achieve  the  same  eﬀect. 
(b)	 (2  points)  Before  we  describe  the  solution,  we  ﬁrst  describe  how  the  dataset  was  created.  This 
may  help  you  appreciate  why  some  feature  mappings  may  work  better  than  others.  The  x  values 
were  created  by  sampling  each  coordinate  uniformly  at  random  from  (-1,1): 
X  in  =  rand(1000,3)*2  - 1 
Given  a  particular  x,  the  corresponding  ytrue  and  ynoisy  values  were  created  as  follows: 
2  − 15 log x2
ytrue  =  −10 log x1
2  − 7.5 log x2  + 2 
3 
� ∼ N (0, 100) 
ynoisy  =  ytrue  + �

To  evaluate  the  performance  of  linear  regression  on  given  training  and  test  sets,  we  created  the 
function  test  linear  reg  which  combines  the  regression,  prediction,  and  evaluation  steps.  You 
may,  of  course,  do  it  in  some  other  way: 

function  err  =  test_linear_reg(y_train,  X_train,  y_test,  X_test)

%  train  linear  regression  using  X_train  and  y_train

%  evaluate  the  mean  squared  prediction  error  on  X_test  and  y_test 


theta  =  linear_regress(y_train,  X_train);

y_hat  =  linear_pred(theta,X_test);

yd  =  y_hat  - y_test;

err  =  sum(yd.^2)/length(yd); 

%err  =  Mean  Squared  Prediction  Error


Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare
(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].(cid:13)(cid:10)

Using  this,  we  can  now  calculate  the  mean  squared  prediction  error  (MSPE)  for  the  two  feature 
mappings: 

>>  X1  =  feature_mapping(X_in,1); 

>>  test_linear_reg(y_noisy,  X1,  y_true,  X1)

ans  =  1.5736e+003

>>  X2  =  feature_mapping(X_in,2); 

>>  test_linear_reg(y_noisy,  X2,  y_true,  X2)

ans  = 
0.5226


The  two  sets  of  errors  are  1573.6  (φ1 )  and  0.5226  (φ2 ),  respectively.  Unsurprisingly,  the  mapping 
φ2  performs much  better  than  φ1–  it  is  exactly  the  space  in which  the  relationship  between X  and 
y  is  linear. 
(c)	 (10 points) Recall from the notes (Eqn 8, lecture 6), that the desired quantity we need to maximize 
is 

vT AAv 
1 + vT Av 
where  A = (XT X)−1 .  In  the  notes,  an  oﬀset  parameter  is  explicitly  assumed  so  that  v = [xT , 1]T . 
However,  in  our  case  this  is  not  necessary  and  so  v = x. 
Active  learning  may,  in  general,  select  the  same  point  x  to  be  observed  repeatedly.  Each  of 
these  observations  corresponds  to  a  diﬀerent  ynoisy .  However,  due  to  practical  limitations,  we  had 
supplied  you with  only  one  set  of  ynoisy  values.  Thus,  if  some xi  occurs  repeatedly  in  idx,  you will 
need  to  use  the  same  ynoisy ,  i  for  each  occurrence  of  xi .  Alternatively,  you  could  change  your  code 
so  as  to  disallow  reptitions  in  idx.  This  is  the  option  we  have  chosen  here. 
Given any feature space, the criterion above will aim to ﬁnd points far apart in that space.  However, 
these  points may  not  be  far  apart  in  the  feature  space  where  classiﬁcation  actually  occurs.  This  is 
of  particular  concern when  the  latter  feature  space might  not  be  easily  accessible  (e.g., when  using 
a  kernel  like  the  radial  basis  function  kernel). 
Here’s  the  active  learning  code: 

function  idx  =  active_learn(X,k1,k2) 

idx  =  1:k1;

N  =  size(X,1);

for  i=1:k2

var_reduction  =  zeros(N,1); 

X1  =  X(idx,:);

A  =  inv(X1’*X1);

AA  =  A*A;

for  j=1:N

if  ismember(j,idx)  %this  is  the  part  where  we  disallow  repititions 
continue;


end

v  =  X(j,:);

a  =  v*AA*v’  /  (  1  +  (v*A*v’));

var_reduction(j)  =  a;


end

[a,  aidx]  =  max(var_reduction); 

idx(end+1)  =  aidx;


end 

Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare
(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].(cid:13)(cid:10)

Using  it,  we  can  now  compute  the  desired  quantities: 

>>  idx1  =  active_learn(X1,5,10) 

idx1 = 
1  2  3  4  5  437  270  928  689  803  670  979  932  558  490

>>  test_linear_reg(y_noisy(idx1),  X2(idx1,:),  y_true,  X2)

ans  =  2.5734e+003

>>  idx2  =  active_learn(X2,5,10)

idx2 = 
1  2  3  4  5  955  803  270  558  628  490  283  592  761  414

>>  test_linear_reg(y_noisy(idx2),  X2(idx2,:),  y_true,  X2)

24.4643

ans  = 

Thus,  the MSPE when using φ1  for active  learning  chooses points  that are not well-placed  (for  this 
particular  dataset);  φ2  performs  much  better.  Note  that  the  feature  mapping  used  to  perform  the 
regression  (and  evaluation)  is  the  same  for  both  (φ2 )  so  the  performance  diﬀerence  is  due  to  the 
points  chosen  by  active  learning. 
The answers change when repititions are allowed (MSPE  for φ1= 117.47  ;  for φ2  = 25.25), but they 
still  illustrate  our  concept.

For  completeness’  sake,  the  answers  for  the  original  version  of  the  problem  are:  (φ1 :  2618.9  (re­

peats),  2618.6  (no  repeats))  and  (φ2 :  25.2546  (no  repeats)  and  24.4643  (repeats)).

(d)	 (4  points)  This  error  will  vary,  depending  upon  the  number  of  iterations  you  perform  and  the 
random points  selected.  In my  simulations,  the value of error  (using mapping φ2  for  regression and 
evaluation) was  33.694.  When  this  simulation was  run  for  a  1000  runs,  the  error was  close  to  25.34 
.  Clearly,  it  seems  to  be  much  better  to  perform  random  sampling  than  perform  active  learning 
in  φ1 ’s  space.  This  may  seem  surprising  at  ﬁrst,  but  is  completely  understandable:  in  the  space 
where  regression  is  performed  (φ2 )  the  points  chosen  by  performing  active  learning  in  the φ1  space 
are  not  far  apart  at  all,  and  are  thus  particularly  bad  points  to  be  sampled. 
For  completeness’  sake,  the  answer  for  the  original  version  of  the  problem  is:  (φ1 :  2218.9  ,  φ2 : 
33.694). 
(e)	 (4  points)  The  ﬁgures  are  shown  in  Fig  1.  For  clarity’s  sake,  we  have  only  plotted  the  last  10 
points  of  idx  (since  the  ﬁrst  5  are  the  same  across  all  cases).

In the original feature space (ﬁg a), the points selected by active learning in the φ1  space are spread

far  apart,  as  expected.  However,  as  part  (b)  showed,  a  better  ﬁt  to  data  is  obtained  by  using  φ2 .

In  this  space  (ﬁg b),  the points  selected by active  learning  in  the φ1  space  are very  closely bunched

together.  The  points  selected  by  active  learning  in  the  φ2  space  are,  in  contrast,  spread  far  apart.

This  helps  explain  why  the  points  learned  using  active  learning  on  the  φ1  space  did  not  lead  to

good  performance  in  the  regression  step.

(a)  5  points  The  function  f (t) =  −β t2/2  monotonically  decreases  with  t  (for  β >  0).  The  function 
g(t) = et  monotonically increases with t.  Thus, the function g o f (t) = h(t) = e−β t2 /2  monotonically 
decreases  with  t.  As  such,  the  RBF  kernel  K (x, x� ) =  e− β 
2 �x−x� �2  deﬁnes  a  Gram  matrix  that 
satisﬁes  the  conditions  of Michelli’s  theorem  and  is  hence  invertible. 
(b)	 5  points  Let A = (λI + K)−1y.  Then, 

lim A = K−1y 
λ 0→
Since K  is  always  invertible,  this  limit  is  always well-deﬁned.  Now,  we  have  ˆαt  = λAt  where At  is 
the  t-th  element  of A.  We  then  have: 

2.	

Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare
(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].(cid:13)(cid:10)

(a)  In  φ1  space	

(b)  In  φ2  space 

We  then  have, 

Figure  1:  The  red  circles  correspond  to  points  chosen  by  performing  active  learning  on  the  φ1  space  and  the

blue  ones  correspond  to  those  chosen  by  performing  active  learning  on  the  φ2  space.

Only  the  last  10  points  of  idx  are  shown  for  each;  the  ﬁrst  5  points  are  the  same. 

� 
y(x) =  � 
y(x) =  �t
n
t=1 ( ˆαt/λ)K (xt , x),  or 
n 
=1 (λAt/λ)K (xt , x),  or 
n 
y(x) = 
=1 AtK (xt , x) 
t
� 
limλ→0 y(x) =  � 
n
t=1 (limλ→0 At )K (xt , x),  or 
� 
t=1 BtK (xt , x),  where  B = K−1y
n
limλ 0 y(x) = 
→
n 
Thus,  in  the  required  limit,  the  function  y(x) = 
=1 BtK (xt , x) 
i
(c)	 10 points To prove that the training error is zero, we need to prove that y(xt ) = yt  for t = 1, . . . , n. 
� 
From  part  (b),  we  have 
i=1 B�iK (xi , xt ),  or 
y(xt ) =  � 
�i=1 � 
n
=1 yj K−1 (i, j ))K (xi , xt )  or
y(xt ) =  � 
n  ( 
n 
y(xt ) = 
j
i=1 K−1 (i, j )K (xi , xt ),  or 
n
n
j=1 yj 
n 
y(xt ) = 
=1 yj δ(j, t),  or 
j
y(xt ) =  yt 
� 
where K−1 (i, j ) = (i, j )-th  entry  of K−1  and 

for  i = j
0 
� 
for  i = j 
1 
Here, we made use of the fact that K (xi , xt ) = K (xt , xi ) and that if A = B−1  then 
i A(t, i)B (i, j ) = 
δ(t, j ). 
(d)	 5  points  Sample  code  for  this  problem  is  shown  below: 

δ(i, j ) = 

Ntrain  =  size(Xtrain,1);

Ntest  =  size(Xtest,1);


Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare
(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].(cid:13)(cid:10)

−101−1−0.8−0.6−0.4−0.200.20.40.60.81−1−0.8−0.6−0.4−0.200.20.40.60.81−12−10−8−6−4−20−16−14−12−10−8−6−4−20−15−10−50�
for  i=1:length(lambda), 
lmb  =  lambda(i); 

alpha  =  lmb  *  ((lmb*eye(Ntrain)  +  K)^-1)  *  Ytrain;

Atrain  =  (1/lmb)  *  repmat(  alpha’,  Ntrain,  1);

yhat_train  =  sum(Atrain.*K,2);


Atest  =  (1/lmb)  *  repmat(  alpha’,  Ntest,  1);

yhat_test  =  sum(Atest.*(Ktrain_test’),  2);


E(i,:)  =  [mean((yhat_train-Ytrain).^2),mean((yhat_test-Ytest).^2)]; 
end; 

Figure  2:  Training  and  test  error  for  Prob #  2(e) 

The resulting plot is shown in Fig 2.  As can be seen the training error is zero at λ = 0 and increases 
as λ increases.  The test error initially decreases, reaches a minimum around 0.1, and then increases 
again.  This  is exactly as we would expect.  λ ≈ 0  results  in over-ﬁtting  (the model  is  too powerful). 
Our  regression  function  has  a  low  bias  but  high  variance.  By  increasing  λ we  constrain  the model, 
thus  increasing  the  training  error.  While  the  regularization  increases  bias,  the  variance  decreases 
faster,  and  we  generalize  better.  High  values  of  λ  result  in  under-ﬁtting  (high  bias,  low  variance) 
and  both  training  error  and  test  errors  are  high. 
� 
3.  (a)  Observe  that  θ  is  only  a  sum  of  ytφ(Xt )’s,  so  we  can  just  store  the  coeﬃcients: 
n
wtytφ(Xt ). 
t=1 
We  can  update  wt ’s  by  incrementing  wt  by  one  when  a mistake  is made  on  example  t.  Classifying 
� 
new  examples  means  evaluating: 
n
wtytK (Xt , X ), 
t=1 

θT φ(X ) = 

θ = 

which  only  involves  kernel  operations.


Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare
(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].(cid:13)(cid:10)

00.10.20.30.40.50.60.70.80.91010203040506070βTrain ErrorTest Error(b)  The  most  straightforward  proof:  use  the  regression  argument  to  show  that  you  can  ﬁt  the  points 
exactly  and  not  only  achieve  the  correct  sign,  but  the  value  of  the  discriminant  function  can  be 
made  ±1  for  every  training  example. 
(c)  Here  is  my  solution. 
%  kernel=‘(1 + transpose(xi )xj )d ’;  d = 5; %  polynomial  kernel 
% kernel=‘exp(−transpose(xi − xj )(xi − xj )/(2s2 ))’;  s = 3; %  radial basis  function kernel 
function  α=train  kernel  perceptron(X ,  y ,  kernel) 
· 
n, d = size(X );
· 
K  =  []; 
· 
for  i = 1 : n
·
· 
xi  = X (i, :)� ;
·
· 
for  j  = 1 : n
·
·
·	
xj  = X (j, :)� ; 
· 
·
·
K (i, j ) = eval(kernel); 
· 
·
end
· 
end
· 
· 
α = zeros(n, 1);
· 
mistakes = 1; 
· 
while  mistakes > 0
·
· 
mistakes = 0; 
· 
· 
for  i = 1 : n
· 
· 
· 
if  α�K (:, i)y(i) ≤ 0 
· 
· 
· 
· 
α(i) = α(i) + y(i); 
· 
· 
·  mistakes = mistakes + 1; 
· 
· 
· 
· 
end 
·
· 
end
· 
end
function  f  =discriminant  function(α,  X ,  kernel,  Xtest ) 
· 
n, d = size(X );
· 
K  =  []; 
· 
for  i = 1 : n
·
· 
xi  = X (i, :)� ;
·
· 
xj  = Xtest ;
·
· 
K (i) = eval(kernel); 
· 
end
· 
f  = α�K ;
(d)  The  original  dataset  requires  d =  4;  the  new  dataset  requires  d =  2.  An  RBF  will  easily  separate 
either  dataset. 
�  load  p3  a 
“X”  and  “y”  loaded. 
�  kernel = ‘(1 + transpose(xi ) ∗ xj )2 ’; 
�  α = train  kernel  perceptron(X, y , kernel); 
�  ﬁgure 
�  hold  on 
�  plot(X (1  : 1000, 1), X (1  : 1000, 2), ‘rs’) 
�  plot(X (1001  : 2000, 1), X (1001  : 2000, 2), ‘bo’) 
� plot  dec	 boundary(α, X, kernel, [−4, −2], [0.5, 0.5], [2, 4]) 

Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare
(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].(cid:13)(cid:10)

�  kernel = ‘exp(−transpose(xi  − xj ) ∗ (xi  − xj )/18)’; 
�  α = train  kernel  perceptron(X, y , kernel); 
�  ﬁgure 
�  hold  on 
�  plot(X (1  : 1000, 1), X (1  : 1000, 2), ‘rs’) 
�  plot(X (1001  : 2000, 1), X (1001  : 2000, 2), ‘bo’) 
� plot  dec  boundary(α, X, kernel, [−4, −2], [0.5, 0.5], [2, 4]) 

Cite as: Tommi Jaakkola, course materials for 6.867 Machine Learning, Fall 2006. MIT OpenCourseWare
(http://ocw.mit.edu/), Massachusetts Institute of Technology. Downloaded on [DD Month YYYY].(cid:13)(cid:10)

−4−3−2−10123−3−2−101234  −3−2−1012−2−10123